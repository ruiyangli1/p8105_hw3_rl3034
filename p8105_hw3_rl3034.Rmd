---
title: "P8105 Homework 3"
author: "Ruiyang Li"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

This is my solution to HW3.

```{r setup, include=FALSE}
library(p8105.datasets)
library(tidyverse)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r instacart_df}
data("instacart")
```

Instacart is an online grocery service that provides online shopping from local stores. For example, in New York City, such local partner stores include Whole Foods, Fairway, and The Food Emporium. Normally, orders will be delivered within 2 hours after placement. 

The Instacart data consists of `r nrow(instacart)` rows of online grocery orders and `r ncol(instacart)` variables from `r length(unique(instacart$user_id))` Instacrat customers. In this dataset, each customer has one or more orders, so the data is in the long format. Some of the key variables include information on customers / orders such as user ID, order ID, order day, and order hour, and information on products such as name, aisle, and department. 


* There are `r length(unique(pull(instacart, aisle_id)))` aisles. The most items are ordered from `r instacart %>% count(aisle) %>% arrange(desc(n)) %>% slice(1) %>% select(aisle) %>% as.character()` aisles, which have `r instacart %>% count(aisle) %>% arrange(desc(n)) %>% slice(1) %>% select(n)` orders. 

```{r collapse=TRUE}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


* The following plot shows the aisles with more than 10000 items ordered. 

```{r collapse=TRUE}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

* The next table shows the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. 

```{r collapse=TRUE}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  arrange(desc(n)) %>% 
  slice(1:3) %>% 
  ungroup() %>% 
  knitr::kable()
```

* The table below shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. 

```{r collapse=TRUE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hr = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hr"
  ) %>% 
  knitr::kable()
```

“The Instacart Online Grocery Shopping Dataset 2017”, Accessed from https://www.instacart.com/datasets/grocery-shopping-2017 on June 24, 2017. 


## Problem 2

* Load, tidy, and wrangle the data. 

```{r clean_accel_df, collapse=TRUE}
accel_df = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
	pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_", 
    values_to = "activity"
  ) %>%
  mutate(
  	minute = as.numeric(minute), 
    day_cat = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday"),
		day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>%
	relocate(week, day_id, day, day_cat)
```

This tidied accelerometer dataset contains `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. These variables include week, day, minute and activity counts for each minute of a 24-hour day starting at midnight. 

* The following table shows the total activities for each day. There are much fewer activities on the Saturdays of week 4 and 5. 

```{r activity_week_day_table, collapse=TRUE}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(activitiy_total = sum(activity)) %>% 
	pivot_wider(
		names_from = "day", 
		values_from = "activitiy_total"
	) %>% 
  knitr::kable()
```

* Here is a single-panel plot that shows the 24-hour activity time courses for each day, colored by day of the week. Based on this graph, it can be seen that Sunday has more activities in the middle of the day; Monday, Wednesday, Friday and Saturday have more activities close to the end of the day; activities are the lowest at the beginning and the end of a day. 

```{r activity_time_course_plt}
accel_df %>% 
	group_by(day, minute) %>% 
  ggplot(aes(x = minute, y = activity, color = day)) + 
	geom_point(alpha = 0.3) + 
	geom_line(alpha = 0.5) + 
	labs(
    title = "24-hour activity time courses",
    x = "Minute of a 24-hour day starting at midnight",
    y = "Activity counts for each minute")
```



## Problem 3

```{r load_ny_noaa_df, collapse=TRUE}
data("ny_noaa")

ny_noaa %>%
  select(-id, -date) %>% 
  summarise_all(list(~sum(is.na(.))))/nrow(ny_noaa)
```

This dataset was acquired from all New York state weather stations from January 1, 1981 through December 31, 2010. It contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Each weather station has multiple dates of observation, so the data is in the long format. There are precipitation variable, snow variables (snowfall and snow depth) and temperature variables (maximum temperature and minimum temperature). There is a lot of missing data in this dataset, which could be an issue. 43.7% data was missing for both temperature variables; 22.8% data on snow depth was missing; about 15% data on snowfall was missing. 

```{r tidy_ny_noaa_df_count_na, collapse=TRUE}
ny_noaa_tidy = 
	ny_noaa %>% 
	separate(date, into = c("year", "month", "day"), sep = "-") %>% 
	mutate(
		prcp = prcp/10,
		tmax = as.numeric(tmax),
		tmax = tmax/10, 
		tmin = as.numeric(tmin),
		tmin = tmin/10, 
		year = as.numeric(year),
		month = month.name[as.numeric(month)], 
		day = as.numeric(day)
	)

ny_noaa_tidy %>% 
	count(snow) %>% 
	arrange(desc(n))
```

* For snowfall, the most commonly observed values are 0 as it occurs 2008508 times. 

* Below is a two-panel plot showing the average max temperature in January and in July in each station across years. It looks that the average max temperatures in 2010 were slightly higher than those in 1981. Also, the average max temperatures in January in 2004 and 1994 are much lower than those in the rest of the years. The average max temperatures in July looks to be less variable than that in January and the temperatures did not change much. There are a few outliers. There is one extreme value in 1981 in January and some others in 1988, 2004, and 2007 in July. There seems to be one station has its values lower than the rest of the stations in July. 

```{r avg_tmax_jan_jul_plt, collapse=TRUE}
avg_tmax_jan_plt = 
	ny_noaa_tidy %>% 
	filter(month %in% c("January")) %>% 
	group_by(id, year, month) %>% 
	summarise(avg_tmax = mean(tmax)) %>% 
	drop_na() %>% 
	ggplot(aes(x = year, y = avg_tmax, group = id, color = id)) + 
	geom_point(alpha = 0.3) + 
	geom_line(alpha = 0.3) +
	labs(
    title = "Average maximum temperature in January in each station across years",
    x = "Year",
    y = "Average maximum temperature (degrees C)") + 
	theme(legend.position = 'none',
        axis.title.x = element_blank())

avg_tmax_jul_plt = 
	ny_noaa_tidy %>% 
	filter(month %in% c("July")) %>% 
	group_by(id, year, month) %>% 
	summarise(avg_tmax = mean(tmax)) %>% 
	drop_na() %>% 
	ggplot(aes(x = year, y = avg_tmax, group = id, color = id)) + 
	geom_point(alpha = 0.3) + 
	geom_line(alpha = 0.3) +
	labs(
    title = "Average maximum temperature in July in each station across years",
    x = "Year",
    y = "Average maximum temperature (degrees C)") + 
	theme(legend.position = 'none',
        axis.title.x = element_blank())

avg_tmax_jan_plt / avg_tmax_jul_plt
```

* The following two-panel plot shows (i) tmax vs tmin for the full dataset; and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r tmax_tmin_snow_plot, collapse=TRUE}
tmin_tmax_p = 
	ny_noaa_tidy %>% 
	ggplot(aes(x = tmin, y = tmax)) + 
	geom_hex() +
	theme(legend.direction = 'vertical', legend.position = "right")

snow_p = 
	ny_noaa_tidy %>% 
	mutate(year = as.character(year)) %>% 
	filter(snow > 0 & snow < 100) %>% 
	ggplot(aes(x = snow, y = year)) +
  geom_density_ridges(scale = .85)

tmin_tmax_p + snow_p
```

These data were accessed from the NOAA National Climatic Data Center, http://doi.org/10.7289/V5D21VHZ, on August 15, 2017.(Menne, M.J., I. Durre, B. Korzeniewski, S. McNeal, K. Thomas, X. Yin, S. Anthony, R. Ray, R.S. Vose, B.E.Gleason, and T.G. Houston, 2012: Global Historical Climatology Network - Daily (GHCN-Daily), Version 3.22.)


